{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\t1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "Filters (kernels) slide over the input image to extract features such as edges, textures, and shapes. The result of applying a filter is called a feature map, which highlights specific patterns in the image. Together, they allow CNNs to learn hierarchical feature representations.\n",
        "***\n",
        "\n",
        " 2. Explain the concepts of padding and stride in CNNs. How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Padding: Adding extra pixels (often zeros) around the input to preserve spatial dimensions.\n",
        "Stride: The step size with which the filter moves across the input.\n",
        "Effects: Larger stride reduces output size. Padding helps maintain output size and prevent information loss.\n",
        "***\n",
        "\n",
        " 3. Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "Receptive field: The region of the input image that influences a particular output neuron.\n",
        "Importance: Larger receptive fields capture more context, enabling deeper CNNs to learn global patterns, not just local features.\n",
        "***\n",
        "\n",
        " 4. Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "Filter size determines the number of weights per filter (f*f*c where c = input channels). Larger filters = more parameters.\n",
        "Stride does not directly affect parameter count, but it reduces output feature map size, which impacts subsequent layers' parameters.\n",
        "***\n",
        "\n",
        " 5. Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "LeNet: Early CNN, shallow (5 layers), small filters (5x5), used for digit recognition.\n",
        "AlexNet: Deeper (8 layers), larger filters (11x11 initially), introduced ReLU and dropout, won ImageNet 2012.\n",
        "VGG: Very deep (16-19 layers), consistent 3x3 filters, improved accuracy, but computationally heavy.\n",
        "***"
      ],
      "metadata": {
        "id": "J1doMVPe_fm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6. Using Keras, build and train a simple CNN model on the MNIST dataset from scratch"
      ],
      "metadata": {
        "id": "Xc5AojO3AGTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1,28,28,1)/255.0\n",
        "X_test = X_test.reshape(-1,28,28,1)/255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdJJTa4JAL6C",
        "outputId": "7c2feb70-3ce6-417e-8722-6bbac966e5b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 21ms/step - accuracy: 0.9144 - loss: 0.2863 - val_accuracy: 0.9811 - val_loss: 0.0588\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9844 - loss: 0.0509 - val_accuracy: 0.9858 - val_loss: 0.0437\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9906 - loss: 0.0303 - val_accuracy: 0.9873 - val_loss: 0.0397\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 25ms/step - accuracy: 0.9938 - loss: 0.0181 - val_accuracy: 0.9865 - val_loss: 0.0417\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.0133 - val_accuracy: 0.9858 - val_loss: 0.0492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2f9b9835c0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images"
      ],
      "metadata": {
        "id": "lb3KlO2eAWmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train, X_test = X_train/255.0, X_test/255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpS-WHvBATor",
        "outputId": "a31ca510-10fa-4266-d4f1-b4ced789eb03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 42ms/step - accuracy: 0.3940 - loss: 1.6577 - val_accuracy: 0.5863 - val_loss: 1.1702\n",
            "Epoch 2/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 41ms/step - accuracy: 0.6094 - loss: 1.1158 - val_accuracy: 0.6510 - val_loss: 0.9979\n",
            "Epoch 3/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 42ms/step - accuracy: 0.6668 - loss: 0.9487 - val_accuracy: 0.6478 - val_loss: 1.0114\n",
            "Epoch 4/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 42ms/step - accuracy: 0.7068 - loss: 0.8391 - val_accuracy: 0.6729 - val_loss: 0.9438\n",
            "Epoch 5/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 42ms/step - accuracy: 0.7382 - loss: 0.7480 - val_accuracy: 0.6842 - val_loss: 0.9326\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a2f9717b350>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 8. Using PyTorch, write a script to define and train a CNN on the MNIST dataset"
      ],
      "metadata": {
        "id": "A8wc-e4FAxDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        # Calculate the output shape after the convolutional layers\n",
        "        self.fc1 = nn.Linear(64 * 24 * 24, 128) # Corrected input size\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(1, 2):\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "L-obE-htA18z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model."
      ],
      "metadata": {
        "id": "rit0B1OsA6iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the directories where your data is stored\n",
        "train_dir = 'data/train'\n",
        "val_dir = 'data/val'\n",
        "\n",
        "# Create ImageDataGenerators for training and validation data\n",
        "# Rescale images to the range [0, 1] and apply data augmentation to the training data\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest') # Add fill_mode for potential gaps after transformations\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255) # Only rescale validation data\n",
        "\n",
        "# Use flow_from_directory to load images from the directories\n",
        "# target_size: The dimensions to which all images will be resized.\n",
        "# batch_size: The number of images to yield from the generator per batch.\n",
        "# class_mode: 'binary' for 2 classes, 'categorical' for more than 2 classes.\n",
        "#            Set to 'binary' if you have only two classes (e.g., 'class1', 'class2' in your directories).\n",
        "#            If you have more than two classes, change this to 'categorical'.\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary') # Change to 'categorical' if you have more than 2 classes\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary') # Change to 'categorical' if you have more than 2 classes\n",
        "\n",
        "\n",
        "# Build a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'), # Added another Conv/MaxPool layer for better feature extraction\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid') # Activation 'sigmoid' for binary classification, 'softmax' for categorical\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "# loss: 'binary_crossentropy' for binary classification, 'categorical_crossentropy' for categorical\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy', # Change to 'categorical_crossentropy' if you have more than 2 classes\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# steps_per_epoch: Total number of steps (batches of samples) before declaring one epoch finished.\n",
        "#                  Usually set to the total number of training samples divided by the batch size.\n",
        "# validation_steps: Total number of steps (batches of samples) to draw before stopping when evaluating on the validation dataset.\n",
        "#                  Usually set to the total number of validation samples divided by the batch size.\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=train_gen.samples // train_gen.batch_size, # Calculate steps per epoch\n",
        "    epochs=10, # Increased epochs for better training\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=val_gen.samples // val_gen.batch_size) # Calculate validation steps\n",
        "\n",
        "# You can access training history for plotting loss and accuracy\n",
        "# history.history['accuracy']\n",
        "# history.history['val_accuracy']\n",
        "# history.history['loss']\n",
        "# history.history['val_loss']"
      ],
      "metadata": {
        "id": "SZ8eeQqKHaHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. End-to-end approach for medical imaging web app (Normal vs Pneumonia).\n",
        "\n",
        "Steps:\n",
        "1. Data preparation: collect X-ray images, preprocess (resize, normalize).\n",
        "2. Model training: CNN with Conv2D, MaxPooling, Dense, dropout.\n",
        "3. Handle imbalance with augmentation.\n",
        "4. Train using Adam and binary crossentropy.\n",
        "5. Save model.\n",
        "6. Deployment: Use Streamlit for web UI, load trained model, allow users to upload X-rays, predict Normal/Pneumonia.\n"
      ],
      "metadata": {
        "id": "EbVnzvd-BCYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example:\n",
        "import streamlit as st\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "model = load_model('xray_cnn.h5')\n",
        "\n",
        "uploaded_file = st.file_uploader('Upload Chest X-ray')\n",
        "if uploaded_file:\n",
        "    img = Image.open(uploaded_file).resize((128,128))\n",
        "    x = np.expand_dims(np.array(img)/255.0, axis=0)\n",
        "    pred = model.predict(x)\n",
        "    st.write('Pneumonia' if pred[0][0]>0.5 else 'Normal')\n"
      ],
      "metadata": {
        "id": "gIQzJ7leHSyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69adcbaa",
        "outputId": "365bd793-20e5-44e8-aee5-58870553f601"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.49.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNIYBdYNGWEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}